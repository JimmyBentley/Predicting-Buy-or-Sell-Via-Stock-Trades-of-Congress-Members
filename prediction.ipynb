{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Buy or Sell via Stock Trades of Congress Members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "\n",
    "### Introduction\n",
    "Objective: Predicting whether a trade is BUY or SELL.\n",
    "\n",
    "Type: Binary classification: there is only BUY (1) or SELL (0).\n",
    "\n",
    "Response Variable: type of transaction (BUY/SELL). We chose this as a indicator to detect market trends.\n",
    "\n",
    "Metric: F1-Score. We are using this because it a good balance between precision and recall. We believe precision and recall are important when predicting whether a trade is a buy or sell. \n",
    "\n",
    "### Baseline Model\n",
    "Made our first Pipeline using the KNNeighborsClassifier model\n",
    "\n",
    "    Features: \n",
    "        Quantitative:\n",
    "            1. Day of transaction \n",
    "            2. Month of transaction\n",
    "            3. Year of transaction        \n",
    "        Nominal:\n",
    "            4. Party\n",
    "        \n",
    "\n",
    "How: For the quantitative features, we used helper functions to strip respective parts of the date string. For 'party', we used one hot encoding to encode categorical values to numerical values.\n",
    "\n",
    "Why: For quantitative columns, buy or sell depends on the date since historical events such as the 2008 recession can cause systematic changes to buying and selling behaviors. For 'party', the buy or sell can depend on party affiliation since the parties could have different buying and selling patterns\n",
    "\n",
    "Model Performance: Using F1-Score for our metric of evaluation, we achieved a score of 0.6520694259012015 on our test set. We are able to conclude that this model is decent, since it's a lot better than the baseline accuracy of 0.5252973381159146 (guessing all 'buy'). Our pipeline improved our F-1 score by 0.12677208778528692.\n",
    "\n",
    "Generalization Ability: Our baseline model ability to generalize on unseen is mediocre since our pipeline only improved by around 13.5 percent. \n",
    "\n",
    "### Final Model\n",
    "\n",
    "New Features: \n",
    "    \n",
    "    Quantitative:\n",
    "        1. Day of disclosure \n",
    "        2. Month of disclosure\n",
    "        3. Year of disclosure\n",
    "        4. est_amount\n",
    "    Nominal:\n",
    "        4. ticker\n",
    "        5. owner\n",
    "        \n",
    "How: For the quantitative features (beside est_amount), we used helper functions to strip respective parts of the date string. For 'ticker', we used one hot encoding to encode categorical values to numerical values. For est_amount, we converted it to z-score based on owner type. \n",
    "\n",
    "Why: For 'ticker', the buy or sell can depend on the performance of a specific stock. For 'owner', buy or sell can depend on the type of stock ownership since there could be different trading behaviors for different groups, such as joint owners buying more than selling. For example, when a company has a risk of being delisted, there is a higher proportion of individuals selling than buying that stock. For quantitative columns (beside est_amount), buy or sell depends on disclosure date since congress members would likely disclose their buys and sales on separate dates. Therefore, we added disclosure dates in addition to transaction date. For est_amount, buy or sell depends on the amount sold due to individuals being more likely to have larger panic sales than panic buys.\n",
    "\n",
    "Generalization Ability: We improved the final model's F1-Score from 0.5252973381159146 to 0.7525264394829612, or about a 23 percent increase. With an F1-Score of 0.7525264394829612, the final model's ability to generalize on unseen data is a lot better than the baseline model. \n",
    "\n",
    "### Fairness Analysis\n",
    "\n",
    "Null Hypothesis: Our model is fair. Its F1-Score for Republicans and Democrats are roughly the same, and differences would be due to random chance.\n",
    "\n",
    "Alternative Hypothesis: Our model is unfair. Its F1-Score for Republicans and Democrats are significantly different.\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "p-value: 0.0\n",
    "\n",
    "Conclusion: We observed a p-value of 0.0, which is less than our alpha of 0.05. Therefore, we have enough evidence to reject our null hypothesis. Therefore, our model is unfairly predicting more accurately for Republicans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in cleaned stock transaction file that we combined with party affiliation from project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('transactions_w_party.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped the index column\n",
    "df = df.drop(['index'], axis = 1)\n",
    "# Dropped the three Libertarian Party member's rows to simplify our model\n",
    "df = df[df['Party']!='Libertarian']\n",
    "# Dropped NaN owner values\n",
    "df['owner'] = df['owner'].fillna('missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplified the four exchange categories, 'purchase', 'sale_partial, 'sale_full', 'exchange' to just buy and sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We dropped exchange\n",
    "df = df[df['type']!='exchange']\n",
    "df['type'] = df['type'].replace({'purchase': 'buy', 'sale_partial': 'sell', 'sale_full': 'sell'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created training and testing data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = df[['transaction_date','Party']]\n",
    "y = df['type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to be used in our function transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns string date into number of days since year 0 for the transaction_date column\n",
    "def num_tdays(df):\n",
    "    return pd.DataFrame(df['transaction_date'].transform(lambda ser: int(ser.split('-')[0]) * 365 + int(ser.split('-')[1]) * 30 + int(ser.split('-')[-1])))\n",
    "# Turns string date into number of days since year 0 for the disclosure_date column\n",
    "def num_ddays(df):\n",
    "    return pd.DataFrame(df['disclosure_date'].transform(lambda ser: int(ser.split('-')[0]) * 365 + int(ser.split('-')[1]) * 30 + int(ser.split('-')[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns string date into day of the month for transaction_date\n",
    "def day(df):\n",
    "    return pd.DataFrame(df['transaction_date'].transform(lambda ser: int(ser.split('-')[-1])))\n",
    "# Turns string date into month for transaction_date\n",
    "def month(df):\n",
    "    return pd.DataFrame(df['transaction_date'].transform(lambda ser: int(ser.split('-')[1])))\n",
    "#Turns string date into year for transaction_date\n",
    "def year(df):\n",
    "    return pd.DataFrame(df['transaction_date'].transform(lambda ser: int(ser.split('-')[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made our first Pipeline using the KNNeighborsClassifier model\n",
    "\n",
    "    Features: \n",
    "        Quantitative:\n",
    "            1. Day of transaction \n",
    "            2. Month of transaction\n",
    "            3. Year of transaction        \n",
    "        Nominal:\n",
    "            4. Party\n",
    "How: For the quantitative features, we used helper functions to strip respective parts of the date string. For 'party', we used one hot encoding to encode categorical values to numerical values.\n",
    "\n",
    "Why: For quantitative columns, buy or sell depends on the date since historical events such as the 2008 recession can cause systematic changes to buying and selling behaviors. For 'party', the buy or sell can depend on party affiliation since the parties could have different buying and selling patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7121951219512195"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column Transformer to encode the four above features\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "    ('day', FunctionTransformer(day), ['transaction_date']),\n",
    "    ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "    ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "    ('nominal', OneHotEncoder(), ['Party'])]\n",
    ")\n",
    "# Pipeline to make combine column transforming and KNN Classifier\n",
    "pl = Pipeline([\n",
    "    # Performs feature engineering \n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('tree', KNeighborsClassifier(n_neighbors=3))\n",
    "])\n",
    "# Fits the training data\n",
    "pl.fit(X_train, y_train)\n",
    "# F1 Score for the training set\n",
    "f1_score(pl.predict(X_train), np.array(y_train),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6520694259012015"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 Score for testing set\n",
    "f1_score(pl.predict(X_test), np.array(y_test),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5248300604229608"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "np.mean(y_train == 'buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12677208778528692"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# improvement \n",
    "0.6520694259012015 - 0.5252973381159146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance:\n",
    "    \n",
    "    Using F1-Score for our metric of evaluation, we achieved a score of 0.6520694259012015. We are able to conclude that this model is decent, since it's a lot better than the baseline accuracy of 0.5252973381159146 (guessing all 'buy'). Our pipeline improved our F-1 score by 0.12677208778528692."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Features: \n",
    "    \n",
    "    Quantitative:\n",
    "        1. Day of disclosure \n",
    "        2. Month of disclosure\n",
    "        3. Year of disclosure\n",
    "        4. est_amount\n",
    "    Nominal:\n",
    "        4. ticker\n",
    "        5. owner\n",
    "        \n",
    "How: For the quantitative features (beside est_amount), we used helper functions to strip respective parts of the date string. For 'ticker', we used one hot encoding to encode categorical values to numerical values. For est_amount, we converted it to z-score based on owner type. \n",
    "\n",
    "Why: For 'ticker', the buy or sell can depend on the performance of a specific stock. For 'owner', buy or sell can depend on the type of stock ownership since there could be different trading behaviors for different groups, such as joint owners buying more than selling. For example, when a company has a risk of being delisted, there is a higher proportion of individuals selling than buying that stock. For quantitative columns (beside est_amount), buy or sell depends on disclosure date since congress members would likely disclose their buys and sales on separate dates. Therefore, we added disclosure dates in addition to transaction date. For est_amount, buy or sell depends on the amount sold due to individuals being more likely to have larger panic sales than panic buys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = df[['transaction_date','est_amount','Party','disclosure_date','ticker', 'owner']]\n",
    "y = df['type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created function to standardized estimated amount by owner type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizes to z-score by group\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# Class to make function\n",
    "class StdScalerByGroup(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.grps_ = dict()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = pd.DataFrame(X)\n",
    "        self.grps_ = dict(X.groupby(X.columns[0]).agg(['mean', 'std']))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        try:\n",
    "            getattr(self, \"grps_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must fit the transformer before tranforming the data!\")\n",
    "        \n",
    "        groups = X.iloc[:, 0].unique()\n",
    "        output = X.iloc[:, 1:].copy()\n",
    "        for c in X.columns[1:]: \n",
    "            total_vals = pd.Series(dtype='float') \n",
    "            for g in groups:\n",
    "                group = X[X.iloc[:, 0] == g][c]\n",
    "                grouped = (group - self.grps_[(c, 'mean')][g]) / self.grps_[(c, 'std')][g]\n",
    "                total_vals = pd.concat([total_vals, pd.Series(grouped)])\n",
    "            output[c] = total_vals\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns string date into day of the month for disclosure_date\n",
    "def dday(df):\n",
    "    return pd.DataFrame(df['disclosure_date'].transform(lambda ser: ser.split('-')[-1]))\n",
    "# Turns string date into month for disclosure_date\n",
    "def dmonth(df):\n",
    "    return pd.DataFrame(df['disclosure_date'].transform(lambda ser: ser.split('-')[1]))\n",
    "#Turns string date into year for disclosure_date\n",
    "def dyear(df):\n",
    "    return pd.DataFrame(df['disclosure_date'].transform(lambda ser: ser.split('-')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.877894570457599"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN ClASSIFIER WITH NEW FEATURES\n",
    "\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "        ('dday', FunctionTransformer(dday), ['disclosure_date']),\n",
    "        ('dmonth', FunctionTransformer(dmonth), ['disclosure_date']),\n",
    "        ('dyear', FunctionTransformer(dyear), ['disclosure_date']),\n",
    "        ('tday', FunctionTransformer(day), ['transaction_date']),\n",
    "        ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "        ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'), ['Party','ticker']),\n",
    "        ('quant', StdScalerByGroup(), ['owner', 'est_amount'])\n",
    "])\n",
    "pl1 = Pipeline([\n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('KN', KNeighborsClassifier(n_neighbors=2))\n",
    "])\n",
    "pl1.fit(X_train, y_train)\n",
    "f1_score(pl1.predict(X_train), np.array(y_train),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7525264394829612"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pl1.predict(X_test), np.array(y_test),pos_label='buy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Model Selection:\n",
    "    We manually checked two other models: DecisionTreeClassifier, and RandomTreeClassifier. Using the same features in each model, we found that our F1-score was the highest using the KNeighborsClassifier, with DecisionTreeClassifier in second, and RandomTreeClassifier in third. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7672526041666667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RANDOM FOREST CLASSIFIER\n",
    "\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "        ('dday', FunctionTransformer(dday), ['disclosure_date']),\n",
    "        ('dmonth', FunctionTransformer(dmonth), ['disclosure_date']),\n",
    "        ('dyear', FunctionTransformer(dyear), ['disclosure_date']),\n",
    "        ('tday', FunctionTransformer(day), ['transaction_date']),\n",
    "        ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "        ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'), ['Party','ticker']),\n",
    "        ('quant', StdScalerByGroup(), ['owner', 'est_amount'])\n",
    "])\n",
    "pl2 = Pipeline([\n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('tree', RandomForestClassifier(max_depth=19))\n",
    "])\n",
    "pl2.fit(X_train, y_train)\n",
    "f1_score(pl2.predict(X_train), np.array(y_train),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6836610827870842"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pl2.predict(X_test), np.array(y_test),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87170626349892"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DECISION TREE CLASSIFIER\n",
    "\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "        ('dday', FunctionTransformer(dday), ['disclosure_date']),\n",
    "        ('dmonth', FunctionTransformer(dmonth), ['disclosure_date']),\n",
    "        ('dyear', FunctionTransformer(dyear), ['disclosure_date']),\n",
    "        ('tday', FunctionTransformer(day), ['transaction_date']),\n",
    "        ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "        ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'), ['Party','ticker']),\n",
    "        ('quant', StdScalerByGroup(), ['owner', 'est_amount'])\n",
    "])\n",
    "pl3 = Pipeline([\n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('tree', DecisionTreeClassifier(max_depth=19))\n",
    "])\n",
    "pl3.fit(X_train, y_train)\n",
    "f1_score(pl3.predict(X_train), np.array(y_train),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7182549987016359"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pl3.predict(X_test), np.array(y_test),pos_label='buy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### GridSearch:\n",
    "    Using GridSearchCV, we found that the best n_neighbors hyperparameter is 1; this overfits the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 ColumnTransformer(transformers=[('dday',\n",
       "                                                  FunctionTransformer(func=<function dday at 0x000001FBC24ACCA0>),\n",
       "                                                  ['disclosure_date']),\n",
       "                                                 ('dmonth',\n",
       "                                                  FunctionTransformer(func=<function dmonth at 0x000001FBA0BEEE50>),\n",
       "                                                  ['disclosure_date']),\n",
       "                                                 ('dyear',\n",
       "                                                  FunctionTransformer(func=<function dyear at 0x000001FBC2669310>),\n",
       "                                                  ['disclosure_date']),\n",
       "                                                 ('tday',\n",
       "                                                  Functi...\n",
       "                                                  ['transaction_date']),\n",
       "                                                 ('month',\n",
       "                                                  FunctionTransformer(func=<function month at 0x000001FBC24ACF70>),\n",
       "                                                  ['transaction_date']),\n",
       "                                                 ('year',\n",
       "                                                  FunctionTransformer(func=<function year at 0x000001FBC2669040>),\n",
       "                                                  ['transaction_date']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['Party', 'ticker']),\n",
       "                                                 ('quant', StdScalerByGroup(),\n",
       "                                                  ['owner', 'est_amount'])])),\n",
       "                ('KN', KNeighborsClassifier(n_neighbors=1))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = {'KN__n_neighbors': np.arange(1,10)}\n",
    "searcher = GridSearchCV(pl1, hyperparameters, scoring='f1_micro')\n",
    "searcher.fit(X_train, y_train)\n",
    "searcher.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Model\n",
    "    Classification Model: K Neighbors Classifier\n",
    "    Parameter: Disclosure date, transaction date, party, ticker, owner, and estimated amount\n",
    "    Hyperparameter: n_neighbors = 2\n",
    "    F1-Score: 0.7525264394829612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.877894570457599"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FINAL MODEL\n",
    "\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "        ('dday', FunctionTransformer(dday), ['disclosure_date']),\n",
    "        ('dmonth', FunctionTransformer(dmonth), ['disclosure_date']),\n",
    "        ('dyear', FunctionTransformer(dyear), ['disclosure_date']),\n",
    "        ('tday', FunctionTransformer(day), ['transaction_date']),\n",
    "        ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "        ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'), ['Party','ticker']),\n",
    "        ('quant', StdScalerByGroup(), ['owner', 'est_amount'])\n",
    "])\n",
    "finalpl = Pipeline([\n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('KN', KNeighborsClassifier(n_neighbors=2))\n",
    "])\n",
    "finalpl.fit(X_train, y_train)\n",
    "f1_score(finalpl.predict(X_train), np.array(y_train), pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7525264394829612"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Model F1 Score\n",
    "f1_score(finalpl.predict(X_test), np.array(y_test), pos_label='buy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis: Our model is fair. Its F1-Score for Republicans and Democrats are roughly the same, and differences would be due to random chance.\n",
    "\n",
    "Alternative Hypothesis: Our model is unfair. Its F1-Score for Republicans and Democrats are significantly different.\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "p-value: 0.0\n",
    "\n",
    "Conclusion: We observed a p-value of 0.0, which is less than our alpha of 0.05. Therefore, we have enough evidence to reject our null hypothesis. Therefore, our model is unfairly predicting more accurately for Republicans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing observed test statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Republican = df[df['Party'] == 'Republican']\n",
    "Democratic = df[df['Party'] == 'Democratic']\n",
    "\n",
    "# train test split REPUBLICAN\n",
    "\n",
    "Xr = Republican[['transaction_date','est_amount','Party','disclosure_date','ticker', 'owner']]\n",
    "yr = Republican['type']\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.25)\n",
    "finalpl.fit(Xr_train, yr_train)\n",
    "\n",
    "r_f1 = f1_score(finalpl.predict(Xr_test), np.array(yr_test),pos_label='buy')\n",
    "\n",
    "# train test split DEMOCRATIC\n",
    "\n",
    "Xd = Democratic[['transaction_date','est_amount','Party','disclosure_date','ticker', 'owner']]\n",
    "yd = Democratic['type']\n",
    "Xd_train, Xd_test, yd_train, yd_test = train_test_split(Xd, yd, test_size=0.25)\n",
    "finalpl.fit(Xd_train, yd_train)\n",
    "\n",
    "d_f1 = f1_score(finalpl.predict(Xd_test), np.array(yd_test),pos_label='buy')\n",
    "\n",
    "# observed test statistics\n",
    "observed_abs_diff = np.abs(r_f1-d_f1)\n",
    "observed_diff = r_f1-d_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11381726813550685"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_abs_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "results = []\n",
    "df_p = df.copy()\n",
    "\n",
    "for _ in range(200):\n",
    "    \n",
    "    # Permutation\n",
    "    \n",
    "    df_p['Shuffled_Party'] = np.random.permutation(df_p['Party'])\n",
    "    Republican = df_p[df_p['Shuffled_Party'] == 'Republican']\n",
    "    Democratic = df_p[df_p['Shuffled_Party'] == 'Democratic']\n",
    "    \n",
    "    # Train test split Republican\n",
    "    \n",
    "    Xr = Republican[['transaction_date','est_amount','Party','disclosure_date','ticker', 'owner']]\n",
    "    yr = Republican['type']\n",
    "    Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.25)\n",
    "    finalpl.fit(Xr_train, yr_train)\n",
    "    # Republican F1-Score\n",
    "    r_f1 = f1_score(finalpl.predict(Xr_test), np.array(yr_test),pos_label='buy')\n",
    "\n",
    "\n",
    "    # Train test split Democratic\n",
    "    \n",
    "    Xd = Democratic[['transaction_date','est_amount','Party','disclosure_date','ticker', 'owner']]\n",
    "    yd = Democratic['type']\n",
    "    Xd_train, Xd_test, yd_train, yd_test = train_test_split(Xd, yd, test_size=0.25)\n",
    "    finalpl.fit(Xd_train, yd_train)\n",
    "    # Democratic F1-Score\n",
    "    d_f1 = f1_score(finalpl.predict(Xd_test), np.array(yd_test),pos_label='buy')\n",
    "\n",
    "    \n",
    "    abs_diff = np.abs(r_f1-d_f1)\n",
    "    \n",
    "    # Test statistics\n",
    "    results.append(abs_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = np.mean(results >= observed_abs_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c748a23e82a04e3aeddf1df0cbe51bfac9883ec65687ad42593cbf64c7ecd29"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
